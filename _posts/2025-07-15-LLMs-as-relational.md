---
title: "LLMs as Relational"
date: 2025-07-15
author: "Mattias Rost"
summary: "Rethinking large language models through a relational view of language where meaning emerges in use, not as representational output."
---
Much of life today is lived in a world of text. From messaging apps and social media feeds to news sites, emails, and search bars, our everyday experience is shaped by language in written form. Language and text permeate our lifeworlds, co-existing with trees, clouds, and bees. In such an environment, it is easy to mistake text as content. Pieces of information to be retrieved, consumed, reposted, liked, and shared. Language becomes data, something that can be gathered and optimized for speed, clarity, and maximum traction.

This can be seen as an industrialization of language. And it is into this regime that LLMs were introduced. These models, trained on massive corpora of digital text, appear to conform to the logic of this world: machines that can produce more of the same, faster, cheaper, and at scale. The dominant metaphor is computational and representational. Users “prompt” the model with questions or instructions and receive a response, an “output,” as if querying a database or consulting an encyclopedia. Text becomes a proxy for something else: information, knowledge, truth. This conceptualization is both intuitive and compelling, but it rests on a problematic understanding of language: as a transparent medium for conveying and carrying representations of the world.

This view mistakes what language is and how meaning arises. It reduces language to a code that transmits facts from one place to another, as though meaning exists outside of and prior to its expression. In such a representational view, language is a mirror or a container that holds something external to itself. But this is not how language fundamentally works, and it is not how we experience meaning.

Instead, we must adopt a richer understanding of language as a medium for _thought_, _relation_, and _emergence_. Meaning is not retrieved from language; it _arises_ through the process of use. When we read, write, or speak, we do not merely decode representations. We participate in the unfolding of sense. And when we interact with systems like LLMs, we do not consult a database; we enter into a kind of dialogue. To treat LLMs as representational tools is to limit the view of language, and missing that language and meaning are interactive processes.

We may turn to hermeneutics for support, in particular Gadamer. For Gadamer, understanding is not the mechanical decoding of information, but a _dialogical_ event, that he called a “fusion of horizons”. Each participant in a dialogue brings a set of pre-understandings, or a _horizon_, shaped by context and past experience. In dialogue, these horizons are not abandoned but brought together. Through the back-and-forth of conversation, a shared understanding takes shape. Not as consensus or equivalence, but as something emergent. Language, for Gadamer, is not simply a tool we use to represent thought. It is the _medium_ in which thought happens, in which understanding becomes possible.

This has deep implications for how we engage with systems like LLMs. If we take their outputs as final, as discrete packages of meaning, we short-circuit the dialogical process. But if we instead treat the interaction with an LLM as a kind of _interpretive encounter_, we can begin to see meaning as relational and temporally unfolding. The model offers a linguistic response, which the human interprets, responds to, rephrases, and continues. The significance is not located in the text alone but in the structure of engagement.

Bakhtin’s philosophy of language complements this view by emphasizing the _social and responsive nature of all utterance_. For Bakhtin, every word is addressed to someone: every expression exists within a web of previous and anticipated responses. Language is inherently dialogic. It does not exist in isolation but always as part of an ongoing interaction. Even a solitary written sentence is shaped by imagined interlocutors, social norms, genres, and the context of utterance. There is no such thing as a neutral or standalone utterance. Every act of language is responsive, positioned, and anticipatory.

Bakhtin’s insight reinforces the idea that when an LLM produces a sentence, its meaning is not fixed in the words themselves, nor does it lie in the model’s training data. Rather, meaning arises in _how_ the user takes up the utterance, how they respond, interpret, and continue the interaction. The model’s outputs are dialogic not because the model understands, but because the _use_ of the model enacts a dialogue. Its sentences exist in the space between user and machine, shaped by anticipation and response. By its very nature, LLMs are trained to produce text in anticipation of human response.

From this vantage point, LLMs are not information engines but _relational technologies_. Their utility lies not in “knowing things” but in enabling a _process_ of inquiry, reflection, and response. This reframing demands a shift in how we relate to both language and machines. Rather than querying for information, we engage in a process of _co-disclosure_: a mutual unfolding of sense that is contextually and temporally situated.

This also aligns with postphenomenological accounts of human-technology relations. Technologies are not neutral conduits of meaning. They mediate our experiences and perceptions of the world, shaping how things appear to us. LLMs do not simply reflect back our queries. They transform the very structure of how we relate to language, information, and ourselves. They invite new patterns of engagement, new rhythms of thought, new forms of dialogue. But only if we treat them as such.

The representational view of information systems, is now embedded in how we conceptualize LLMs and language. It risks flattening the rich, dialogical, and emergent nature of meaning. We must resist this flattening, and reclaim language as a medium of thought, before our use of LLMs becomes sedimented into patterns of retrieval and output. LLMs, far from being oracle-machines, are strange and powerful interlocutors. Their value lies not in what they _say_, but in what we can _do_ with what they offer, in the _relational space_ that opens up between prompt and response. Meaning, in this view, is not something we extract. It is something we participate in.
